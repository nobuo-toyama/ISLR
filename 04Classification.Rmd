---
title: "4 Classification"
author: "nobuo"
date: "2021/3/30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lab: Logistic Regression, LDA, QDA, and KNN

### The Stock Market Data

```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
```
```{r}
cor(Smarket[ ,-9])
```



```{r}
```
```{r}
plot(Smarket$Volume)
```
### Logistic Regression
```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = Smarket)
summary(glm.fits)
coef(glm.fits)
```
```{r}
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Smarket$Direction)
```
The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.

```{r}
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > 0.5] <- "Up"
```
Given these predictions, the `table()` function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.
```{r}
table(glm.pred, Smarket$Direction)
(145 + 507)/1250
mean(glm.pred==Smarket$Direction)

```
To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.

```{r}
train <- Smarket$Year < 2005
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Direction.2005 <- Smarket$Direction[!train]
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = Smarket, subset = train)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")

glm.pred <- rep("Down", 252)
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
```
```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2, family = binomial, data = Smarket, subset = train)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```
### Linear Discriminant Analysis

```{r}
library(MASS)
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda.fit
plot(lda.fit)
```
The `predict()` function returns a list with three elements.
```{r}
lda.pred <- predict(lda.fit, Smarket.2005)
names((lda.pred))
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```
Applying a 50 % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.
```{r}
sum(lda.pred$posterior[ ,1] >= 0.5)
sum(lda.pred$posterior[ ,1] < 0.5)
```
###  Quadratic Discriminant Analysis
```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda.fit
```
```{r}
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
```
### K-Nearest Neighbors

We use the `cbind()` function, short for column bind, to bind the `Lag1` and `cbind()`
`Lag2` variables together into two matrices, one for the training set and the
other for the test set.
```{r}
library(class)
train.X <- cbind(Smarket$Lag1, Smarket$Lag2)[train, ]
test.X <- cbind(Smarket$Lag1, Smarket$Lag2)[!train, ]
train.Direction <- Smarket$Direction[train]
```

```{r}
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```
```{r}
knn.pred <- knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```
### An Application to Caravan Insurance Data

```{r}
dim(Caravan)
attach(Caravan)
summary(Purchase)

```
A good way to handle this problem is to *standardize* the data so that all standardize
variables are given a mean of zero and a standard deviation of one. 
 In standardizing the data, we exclude column 86, because that is the qualitative `Purchase` variable.
 
```{r}
standerdized.X <- scale(Caravan[, -86])
var(Caravan[,1])
var(Caravan[,2])
var(standerdized.X[,1])
var(standerdized.X[,2])
```
We now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using $K = 1$, and evaluate its performance on the test data.
```{r}
test <- 1:1000
train.X <- standerdized.X[-test, ]
test.X <- standerdized.X[test, ]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
```

 

